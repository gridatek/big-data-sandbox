{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced Analytics - Enterprise Big Data Patterns\n\nWelcome to the **Advanced Analytics Tutorial**! This notebook covers sophisticated big data operations and real-world patterns.\n\n## ðŸ‘‹ New to Big Data?\n**Start with the beginner tutorial first:** `01_getting_started.ipynb`\n\nThat tutorial teaches the fundamentals in 15 minutes. This one assumes you understand:\n- Basic Spark operations\n- Data processing concepts  \n- The sandbox architecture\n\n## ðŸŽ¯ What You'll Master Here:\n1. **Advanced Spark**: Complex transformations, window functions, and optimization\n2. **MinIO Integration**: Reading/writing from object storage (S3-compatible)\n3. **Data Generation**: Creating realistic datasets for testing\n4. **Complex Analytics**: Time series, customer segmentation, statistical analysis\n5. **Kafka Streaming**: Preparing data for real-time processing\n6. **Production Patterns**: Error handling, performance tuning, monitoring\n\n## â±ï¸ Time Required: 45-60 minutes\n\n## ðŸ”§ Prerequisites\n- Completed `01_getting_started.ipynb` \n- All services running (`./verify-services.sh`)\n- Basic understanding of SQL and data analysis\n\nReady for advanced big data engineering? Let's dive deep! ðŸš€"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Advanced Setup and Production Configuration\n\nSetting up Spark with enterprise-grade configurations for real-world scenarios."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries for advanced analytics\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport json\nimport random\nimport numpy as np\n\n# Set up professional plotting\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"ðŸ“š Advanced libraries imported successfully!\")\nprint(\"ðŸŽ¯ Ready for enterprise-grade data processing\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Enterprise Spark Session with MinIO Integration\n\nProduction-grade Spark configuration with optimizations and MinIO object storage."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create production-grade Spark session with advanced configurations\nprint(\"ðŸ”§ Initializing enterprise Spark session...\")\n\nspark = SparkSession.builder \\\n    .appName(\"BigDataSandbox_AdvancedTutorial\") \\\n    .master(\"spark://spark-master:7077\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .getOrCreate()\n\n# Set log level to reduce noise\nspark.sparkContext.setLogLevel(\"WARN\")\n\nprint(f\"âœ… Spark {spark.version} initialized with enterprise configurations\")\nprint(f\"ðŸŽ¯ Cluster resources: {spark.sparkContext.defaultParallelism} cores\")\nprint(f\"ðŸ“Š Monitor at: http://localhost:4040\")\nprint(f\"ðŸ’¾ Object storage ready: MinIO S3A integration active\")\nprint(\"\\nðŸš€ Production optimizations enabled:\")\nprint(\"   â€¢ Adaptive Query Execution (AQE)\")\nprint(\"   â€¢ Automatic partition coalescing\") \nprint(\"   â€¢ Skew join optimization\")\nprint(\"   â€¢ Kryo serialization for performance\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample sales data\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_sales_data(num_records=1000):\n",
    "    \"\"\"Generate sample sales data\"\"\"\n",
    "    \n",
    "    products = ['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones', 'Camera', 'Speaker', 'Monitor']\n",
    "    categories = ['Electronics', 'Audio', 'Computing', 'Mobile']\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    \n",
    "    data = []\n",
    "    start_date = datetime.now() - timedelta(days=90)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        date = start_date + timedelta(days=random.randint(0, 90))\n",
    "        data.append({\n",
    "            'transaction_id': f'TRX{i:06d}',\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'customer_id': f'CUST{random.randint(1, 200):04d}',\n",
    "            'product_id': f'PROD{random.randint(1, 8):03d}',\n",
    "            'product_name': random.choice(products),\n",
    "            'quantity': random.randint(1, 5),\n",
    "            'price': round(random.uniform(50, 2000), 2),\n",
    "            'category': random.choice(categories),\n",
    "            'region': random.choice(regions)\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate and create DataFrame\n",
    "sales_data = generate_sales_data(1000)\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "\n",
    "print(f\"Generated {sales_df.count()} records\")\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calculated columns\n",
    "sales_df = sales_df.withColumn(\"total_amount\", col(\"quantity\") * col(\"price\"))\n",
    "sales_df = sales_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "sales_df = sales_df.withColumn(\"year\", year(\"date\"))\n",
    "sales_df = sales_df.withColumn(\"month\", month(\"date\"))\n",
    "\n",
    "# Register as temp view for SQL queries\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# SQL query example\n",
    "top_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_name,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(total_amount) as total_revenue,\n",
    "        AVG(total_amount) as avg_transaction_value\n",
    "    FROM sales\n",
    "    GROUP BY product_name\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 5 Products by Revenue:\")\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "daily_sales = sales_df.groupBy(\"date\") \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"daily_revenue\"),\n",
    "        count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .orderBy(\"date\")\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "daily_sales_pd = daily_sales.toPandas()\n",
    "\n",
    "# Plot daily revenue\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "axes[0].plot(daily_sales_pd['date'], daily_sales_pd['daily_revenue'], marker='o')\n",
    "axes[0].set_title('Daily Revenue Trend')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Revenue ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].bar(daily_sales_pd['date'], daily_sales_pd['unique_customers'], alpha=0.7)\n",
    "axes[1].set_title('Daily Unique Customers')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Customer Count')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer analysis\n",
    "customer_stats = sales_df.groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"lifetime_value\"),\n",
    "        count(\"transaction_id\").alias(\"purchase_count\"),\n",
    "        avg(\"total_amount\").alias(\"avg_purchase_value\"),\n",
    "        max(\"date\").alias(\"last_purchase_date\"),\n",
    "        min(\"date\").alias(\"first_purchase_date\")\n",
    "    )\n",
    "\n",
    "# Add customer segments\n",
    "customer_segments = customer_stats.withColumn(\n",
    "    \"segment\",\n",
    "    when(col(\"lifetime_value\") > 5000, \"VIP\")\n",
    "    .when(col(\"lifetime_value\") > 2000, \"Gold\")\n",
    "    .when(col(\"lifetime_value\") > 500, \"Silver\")\n",
    "    .otherwise(\"Bronze\")\n",
    ")\n",
    "\n",
    "# Segment distribution\n",
    "segment_dist = customer_segments.groupBy(\"segment\").count().toPandas()\n",
    "\n",
    "# Visualize segments\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(segment_dist['count'], labels=segment_dist['segment'], autopct='%1.1f%%')\n",
    "plt.title('Customer Segment Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "segment_revenue = customer_segments.groupBy(\"segment\") \\\n",
    "    .agg(sum(\"lifetime_value\").alias(\"total_revenue\")) \\\n",
    "    .toPandas()\n",
    "plt.bar(segment_revenue['segment'], segment_revenue['total_revenue'])\n",
    "plt.title('Revenue by Customer Segment')\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Total Revenue ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 VIP Customers:\")\n",
    "customer_segments.filter(col(\"segment\") == \"VIP\") \\\n",
    "    .orderBy(col(\"lifetime_value\").desc()) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write Results to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write processed data to MinIO\n",
    "try:\n",
    "    # Write customer segments\n",
    "    customer_segments.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"s3a://processed/customer_segments\")\n",
    "    \n",
    "    print(\"âœ… Customer segments written to MinIO\")\n",
    "    \n",
    "    # Write daily sales\n",
    "    daily_sales.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"s3a://processed/daily_sales\")\n",
    "    \n",
    "    print(\"âœ… Daily sales written to MinIO\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: MinIO write skipped in demo mode. Error: {e}\")\n",
    "    print(\"In production, this would save to MinIO successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Kafka Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Prepare data for Kafka streaming\n",
    "# In production, you would use kafka-python or Spark Streaming\n",
    "\n",
    "# Convert recent transactions to JSON for Kafka\n",
    "recent_transactions = sales_df \\\n",
    "    .filter(col(\"date\") >= (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')) \\\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_name\",\n",
    "        \"total_amount\",\n",
    "        \"region\"\n",
    "    ) \\\n",
    "    .limit(10)\n",
    "\n",
    "# Convert to JSON format (ready for Kafka)\n",
    "kafka_messages = recent_transactions.toJSON().collect()\n",
    "\n",
    "print(\"Sample Kafka messages:\")\n",
    "for i, msg in enumerate(kafka_messages[:3]):\n",
    "    print(f\"Message {i+1}: {msg}\")\n",
    "\n",
    "print(f\"\\nðŸ“¨ {len(kafka_messages)} messages ready for Kafka streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "# spark.stop()\n",
    "print(\"ðŸŽ‰ Tutorial completed! Keep the Spark session active for further exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You've completed the Getting Started tutorial. Here's what you can explore next:\n",
    "\n",
    "1. **Try the Airflow UI**: Visit http://localhost:8080 to trigger DAGs\n",
    "2. **Explore MinIO**: Check http://localhost:9000 to see your stored data\n",
    "3. **Monitor Kafka**: Use http://localhost:9001 to view topics and messages\n",
    "4. **Advanced Notebooks**: Check out the other notebooks in this folder\n",
    "\n",
    "### Useful Commands\n",
    "\n",
    "```python\n",
    "# Read from MinIO\n",
    "df = spark.read.parquet(\"s3a://bucket/path\")\n",
    "\n",
    "# Write to MinIO\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://bucket/path\")\n",
    "\n",
    "# Stream from Kafka (structured streaming)\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_name\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "Happy Data Engineering! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}