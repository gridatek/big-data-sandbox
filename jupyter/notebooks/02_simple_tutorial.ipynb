{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Big Data Tutorial - Your First Pipeline\n",
    "\n",
    "Welcome! This is a beginner-friendly introduction to the Big Data Sandbox.\n",
    "\n",
    "## What we'll do:\n",
    "1. ‚úÖ Check if everything is working\n",
    "2. üìä Load some sample data\n",
    "3. üîÑ Process it with Spark\n",
    "4. üíæ Save the results\n",
    "5. üìà Make a simple chart\n",
    "\n",
    "**No experience needed!** Just follow along step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Check Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tools we need\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark (this might take a moment)\n",
    "print(\"üîÑ Connecting to Spark...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTutorial\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Connected to Spark version {spark.version}\")\n",
    "print(f\"üìä You can see Spark jobs at: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Some Sample Data\n",
    "\n",
    "Let's create a simple dataset about online store sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = [\n",
    "    (\"Phone\", \"Electronics\", 599.99, 5),\n",
    "    (\"Laptop\", \"Electronics\", 1299.99, 3),\n",
    "    (\"Book\", \"Education\", 19.99, 15),\n",
    "    (\"Headphones\", \"Electronics\", 199.99, 8),\n",
    "    (\"Desk\", \"Furniture\", 299.99, 2),\n",
    "    (\"Chair\", \"Furniture\", 149.99, 4),\n",
    "    (\"Notebook\", \"Education\", 4.99, 25),\n",
    "    (\"Mouse\", \"Electronics\", 29.99, 12)\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"product\", \"category\", \"price\", \"quantity_sold\"]\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"‚úÖ Sample data created!\")\n",
    "print(\"Here's what our data looks like:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process the Data\n",
    "\n",
    "Let's calculate some basic business metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column: total revenue per product\n",
    "df_with_revenue = df.withColumn(\"total_revenue\", col(\"price\") * col(\"quantity_sold\"))\n",
    "\n",
    "print(\"üí∞ Added revenue calculation:\")\n",
    "df_with_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best-selling products\n",
    "top_products = df_with_revenue.orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"üèÜ Products ranked by total revenue:\")\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category to see which category performs best\n",
    "category_summary = df_with_revenue.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "        sum(\"quantity_sold\").alias(\"total_items_sold\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"category_revenue\").desc())\n",
    "\n",
    "print(\"üìä Sales by category:\")\n",
    "category_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a Simple Chart\n",
    "\n",
    "Let's visualize our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easy plotting\n",
    "category_data = category_summary.toPandas()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_data['category'], category_data['category_revenue'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Revenue by Category', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(category_data['category_revenue']):\n",
    "    plt.text(i, v + 50, f'${v:,.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Chart created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Summary\n",
    "\n",
    "Let's see what we learned from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some final insights\n",
    "total_revenue = df_with_revenue.agg(sum(\"total_revenue\")).collect()[0][0]\n",
    "total_items = df_with_revenue.agg(sum(\"quantity_sold\")).collect()[0][0]\n",
    "avg_order_value = total_revenue / df_with_revenue.count()\n",
    "\n",
    "print(\"üìä BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"üí∞ Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"üì¶ Total Items Sold: {total_items}\")\n",
    "print(f\"üõí Average Order Value: ${avg_order_value:.2f}\")\n",
    "print(\"\\nüèÜ Key Findings:\")\n",
    "print(\"- Electronics is our top-performing category\")\n",
    "print(\"- Laptops generate the highest individual revenue\")\n",
    "print(\"- Education products sell in higher quantities but at lower prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Your Work (Optional)\n",
    "\n",
    "Let's save our processed data to MinIO (our data storage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This would normally save to MinIO, but we'll skip it for this demo\n",
    "# In a real scenario, you would uncomment the lines below:\n",
    "\n",
    "# df_with_revenue.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"s3a://processed/sales_analysis\")\n",
    "\n",
    "print(\"üíæ In a real scenario, your data would now be saved to MinIO!\")\n",
    "print(\"üìÅ You could view it at: http://localhost:9000\")\n",
    "print(\"üîç Files would appear in the 'processed' bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed your first big data pipeline! Here's what you accomplished:\n",
    "\n",
    "‚úÖ **Connected to Spark** - You're now using distributed computing!  \n",
    "‚úÖ **Processed data** - You transformed raw data into insights  \n",
    "‚úÖ **Created visualizations** - You made data easy to understand  \n",
    "‚úÖ **Generated insights** - You discovered meaningful business information  \n",
    "\n",
    "## What's Next?\n",
    "\n",
    "1. **Try the Advanced Tutorial**: Open `01_getting_started.ipynb` for more complex examples\n",
    "2. **Explore Real Data**: Use the sample CSV files in the `/data` folder\n",
    "3. **Stream Data**: Check out the Kafka streaming examples\n",
    "4. **Build Pipelines**: Create automated workflows with Airflow\n",
    "\n",
    "## Need Help?\n",
    "\n",
    "- **Airflow UI**: http://localhost:8080 (workflow management)\n",
    "- **Spark UI**: http://localhost:4040 (job monitoring)\n",
    "- **MinIO Console**: http://localhost:9000 (data storage)\n",
    "- **Kafka UI**: http://localhost:9001 (streaming data)\n",
    "\n",
    "Happy data engineering! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (optional)\n",
    "# spark.stop()\n",
    "print(\"üéØ Tutorial complete! Keep exploring!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}