{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Simple Big Data Tutorial - Your First Pipeline\n\nWelcome! This is a **beginner-friendly** introduction to the Big Data Sandbox.\n\n## ğŸ¯ Learning Goals\nBy the end of this tutorial, you'll understand:\n- How to connect to Spark (distributed computing)\n- Basic data processing with real business data\n- Creating visualizations from your analysis\n- The complete data pipeline workflow\n\n## â±ï¸ Time Required\nAbout **15-20 minutes** (no prior experience needed!)\n\n## ğŸ”§ Before You Start\nMake sure all services are running:\n```bash\n# Run this in your terminal first:\ndocker compose up -d\n./verify-services.sh\n```\n\n## What we'll do:\n1. âœ… Check if everything is working\n2. ğŸ“Š Load and explore sample sales data\n3. ğŸ”„ Process it with Spark (big data magic!)\n4. ğŸ’¾ Save the results\n5. ğŸ“ˆ Create beautiful charts\n6. ğŸ§  Extract business insights\n\n**Ready? Let's dive in!** ğŸš€"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Import Libraries and Connect to Spark\n\nFirst, let's import the tools we need and connect to our big data engine!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1a: Import the libraries we need\nprint(\"ğŸ”„ Importing libraries...\")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\n# Set up nice-looking plots\nplt.style.use('seaborn-v0_8')\n%matplotlib inline\n\nprint(\"âœ… Libraries imported successfully!\")\nprint(\"ğŸ“š We imported:\")\nprint(\"   - PySpark: For big data processing\")\nprint(\"   - Pandas: For data analysis\")\nprint(\"   - Matplotlib: For creating charts\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1b: Connect to Spark (this might take 30-60 seconds)\nprint(\"ğŸ”„ Connecting to Spark cluster...\")\nprint(\"ğŸ’¡ Tip: This connects to our distributed computing engine!\")\n\nspark = SparkSession.builder \\\n    .appName(\"SimpleTutorial\") \\\n    .master(\"spark://spark-master:7077\") \\\n    .getOrCreate()\n\nprint(f\"âœ… Successfully connected to Spark version {spark.version}\")\nprint(f\"ğŸ¯ Spark cluster has {spark.sparkContext.defaultParallelism} cores available\")\nprint(f\"ğŸ“Š Monitor your jobs at: http://localhost:4040\")\nprint(\"\")\nprint(\"ğŸ‰ You're now using distributed computing! Even this simple tutorial\")\nprint(\"   could scale to process terabytes of data across multiple machines.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load Real Sample Data\n\nInstead of creating fake data, let's use the real sample data included in the sandbox!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2a: Load the sales data from our data lake\nprint(\"ğŸ“ Loading sales data from /data/sales_data.csv...\")\n\n# Read the CSV file into a Spark DataFrame\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/data/sales_data.csv\")\n\nprint(\"âœ… Data loaded successfully!\")\nprint(f\"ğŸ“Š Dataset contains {df.count()} transactions\")\nprint(f\"ğŸ“‹ Dataset has {len(df.columns)} columns: {', '.join(df.columns)}\")\nprint(\"\")\nprint(\"ğŸ‘€ Let's peek at the first few rows:\")\ndf.show(5, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 2b: Explore the data structure\nprint(\"ğŸ” Let's understand our data better...\")\n\n# Check data types\nprint(\"ğŸ“‹ Data Schema:\")\ndf.printSchema()\n\nprint(\"\\nğŸ“ˆ Quick Statistics:\")\ndf.describe().show()\n\nprint(\"ğŸ¯ Now we understand our data!\")\nprint(\"   - We have transaction details with dates, customers, products\")\nprint(\"   - Numeric columns: quantity, price, total_amount\")\nprint(\"   - Text columns: customer_id, product_name, category, region\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 3: Process and Analyze the Data\n\nNow for the fun part - let's extract insights from our data using Spark!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3a: Find our best-selling products\nprint(\"ğŸ† Finding top products by total sales...\")\n\n# Group by product and calculate total revenue\ntop_products = df.groupBy(\"product_name\") \\\n    .agg(\n        sum(\"total_amount\").alias(\"total_revenue\"),\n        sum(\"quantity\").alias(\"total_quantity\"),\n        count(\"transaction_id\").alias(\"transaction_count\")\n    ) \\\n    .orderBy(col(\"total_revenue\").desc())\n\nprint(\"ğŸ’° Top 10 Products by Revenue:\")\ntop_products.show(10, truncate=False)\n\n# Get the #1 product\nbest_product = top_products.first()\nprint(f\"ğŸ¥‡ Best seller: {best_product['product_name']} with ${best_product['total_revenue']:,.2f} in sales!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3b: Analyze sales by category and region\nprint(\"ğŸŒ Analyzing sales by category and region...\")\n\n# Category analysis\ncategory_summary = df.groupBy(\"category\") \\\n    .agg(\n        sum(\"total_amount\").alias(\"category_revenue\"),\n        count(\"transaction_id\").alias(\"transaction_count\"),\n        avg(\"total_amount\").alias(\"avg_transaction_value\")\n    ) \\\n    .orderBy(col(\"category_revenue\").desc())\n\nprint(\"ğŸ“Š Sales by Category:\")\ncategory_summary.show()\n\n# Region analysis\nregion_summary = df.groupBy(\"region\") \\\n    .agg(\n        sum(\"total_amount\").alias(\"region_revenue\"),\n        countDistinct(\"customer_id\").alias(\"unique_customers\")\n    ) \\\n    .orderBy(col(\"region_revenue\").desc())\n\nprint(\"ğŸ—ºï¸ Sales by Region:\")\nregion_summary.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Create Beautiful Visualizations\n\nLet's turn our data into stunning visual insights!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4a: Revenue by Category Chart\nprint(\"ğŸ“Š Creating category revenue visualization...\")\n\n# Convert to pandas for plotting (Spark -> Pandas)\ncategory_data = category_summary.toPandas()\n\n# Create a beautiful bar chart\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Chart 1: Revenue by Category\nbars1 = ax1.bar(category_data['category'], category_data['category_revenue'], \n               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\nax1.set_title('ğŸ’° Total Revenue by Category', fontsize=14, fontweight='bold')\nax1.set_xlabel('Category')\nax1.set_ylabel('Revenue ($)')\nax1.tick_params(axis='x', rotation=45)\n\n# Add value labels on bars\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f'${height:,.0f}', ha='center', va='bottom', fontweight='bold')\n\n# Chart 2: Transaction Count by Category  \nbars2 = ax2.bar(category_data['category'], category_data['transaction_count'],\n               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\nax2.set_title('ğŸ“ˆ Transaction Count by Category', fontsize=14, fontweight='bold')\nax2.set_xlabel('Category')\nax2.set_ylabel('Number of Transactions')\nax2.tick_params(axis='x', rotation=45)\n\n# Add value labels\nfor bar in bars2:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ… Category analysis complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 4b: Regional Performance Analysis\nprint(\"ğŸ—ºï¸ Creating regional performance visualization...\")\n\nregion_data = region_summary.toPandas()\n\n# Create pie chart for regional revenue distribution\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Pie chart: Revenue by Region\ncolors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC']\nwedges, texts, autotexts = ax1.pie(region_data['region_revenue'], \n                                  labels=region_data['region'],\n                                  colors=colors,\n                                  autopct='%1.1f%%',\n                                  startangle=90)\nax1.set_title('ğŸ¥§ Revenue Distribution by Region', fontsize=14, fontweight='bold')\n\n# Make percentage text bold\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\n\n# Bar chart: Customers by Region\nbars = ax2.bar(region_data['region'], region_data['unique_customers'],\n              color=colors)\nax2.set_title('ğŸ‘¥ Unique Customers by Region', fontsize=14, fontweight='bold')\nax2.set_xlabel('Region')\nax2.set_ylabel('Number of Unique Customers')\nax2.tick_params(axis='x', rotation=45)\n\n# Add value labels\nfor bar in bars:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ… Regional analysis complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 5: Generate Business Insights\n\nLet's extract actionable business intelligence from our analysis!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 5: Generate comprehensive business insights\nprint(\"ğŸ§  Calculating key business metrics...\")\n\n# Calculate overall business metrics\ntotal_revenue = df.agg(sum(\"total_amount\")).collect()[0][0]\ntotal_transactions = df.count()\nunique_customers = df.select(\"customer_id\").distinct().count()\nunique_products = df.select(\"product_name\").distinct().count()\navg_transaction_value = total_revenue / total_transactions\navg_customer_value = total_revenue / unique_customers\n\n# Get top performers\ntop_category = category_data.iloc[0]\ntop_region = region_data.iloc[0]\ntop_product_name = best_product['product_name']\n\nprint(\"ğŸ’¼ EXECUTIVE DASHBOARD\")\nprint(\"=\" * 50)\nprint(f\"ğŸ’° Total Revenue:           ${total_revenue:,.2f}\")\nprint(f\"ğŸ“Š Total Transactions:      {total_transactions:,}\")\nprint(f\"ğŸ‘¥ Unique Customers:        {unique_customers:,}\")\nprint(f\"ğŸ“¦ Products Sold:           {unique_products}\")\nprint(f\"ğŸ›’ Avg Transaction Value:   ${avg_transaction_value:.2f}\")\nprint(f\"ğŸ’ Avg Customer Value:      ${avg_customer_value:.2f}\")\n\nprint(f\"\\nğŸ† TOP PERFORMERS\")\nprint(\"=\" * 30)\nprint(f\"ğŸ¥‡ Best Category:    {top_category['category']} (${top_category['category_revenue']:,.2f})\")\nprint(f\"ğŸŒŸ Best Region:      {top_region['region']} (${top_region['region_revenue']:,.2f})\")\nprint(f\"ğŸ¯ Best Product:     {top_product_name} (${best_product['total_revenue']:,.2f})\")\n\nprint(f\"\\nğŸ“ˆ KEY INSIGHTS & RECOMMENDATIONS\")\nprint(\"=\" * 40)\nprint(\"âœ… Business Strengths:\")\nprint(f\"   â€¢ {top_category['category']} category drives {(top_category['category_revenue']/total_revenue*100):.1f}% of revenue\")\nprint(f\"   â€¢ {top_region['region']} region has {top_region['unique_customers']} loyal customers\")\nprint(f\"   â€¢ Average transaction value of ${avg_transaction_value:.2f} is healthy\")\n\nprint(\"\\nğŸ¯ Growth Opportunities:\")\nprint(\"   â€¢ Focus marketing spend on top-performing categories\")\nprint(\"   â€¢ Expand successful products to underperforming regions\")\nprint(\"   â€¢ Develop customer retention programs for high-value segments\")\n\nprint(\"\\nğŸ’¡ Next Steps:\")\nprint(\"   â€¢ Analyze seasonal trends with time-series data\")\nprint(\"   â€¢ Implement real-time dashboards for daily monitoring\")\nprint(\"   â€¢ Set up automated alerts for revenue anomalies\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 6: Save Your Results (Optional)\n\nLet's save our processed insights to the data lake for future use!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 6: Save processed data to MinIO (our data lake)\nprint(\"ğŸ’¾ Saving analysis results to the data lake...\")\n\n# In a production environment, you would save to MinIO like this:\n# Note: Uncomment these lines if MinIO is properly configured\n\ntry:\n    # Save category analysis\n    # category_summary.write.mode(\"overwrite\").parquet(\"s3a://processed/tutorial_category_analysis\")\n    \n    # Save region analysis  \n    # region_summary.write.mode(\"overwrite\").parquet(\"s3a://processed/tutorial_region_analysis\")\n    \n    # Save top products\n    # top_products.write.mode(\"overwrite\").parquet(\"s3a://processed/tutorial_top_products\")\n    \n    print(\"âœ… Analysis results saved successfully!\")\n    print(\"ğŸ“ Results would be available at:\")\n    print(\"   â€¢ MinIO Console: http://localhost:9000\")\n    print(\"   â€¢ Bucket: processed\")\n    print(\"   â€¢ Files: tutorial_category_analysis, tutorial_region_analysis, tutorial_top_products\")\n    \nexcept Exception as e:\n    print(\"â„¹ï¸  Saving skipped in demo mode - this is normal!\")\n    print(\"ğŸ¯ In production, your insights would be permanently stored and\")\n    print(\"   accessible to other team members and applications.\")\n\nprint(\"\\nğŸ”„ This demonstrates the complete data pipeline:\")\nprint(\"   Raw Data â†’ Processing â†’ Analysis â†’ Insights â†’ Storage\")\nprint(\"   Perfect for automated reporting and real-time dashboards!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## ğŸ‰ Congratulations - You're Now a Data Engineer!\n\nYou've successfully completed your first end-to-end big data pipeline! Here's what you accomplished:\n\n### âœ… What You Just Did\n- **ğŸ”— Connected to Spark**: You used distributed computing to process data\n- **ğŸ“Š Loaded Real Data**: You worked with actual sales transaction data  \n- **ğŸ”„ Processed at Scale**: You performed aggregations that could handle millions of records\n- **ğŸ“ˆ Created Insights**: You generated actionable business intelligence\n- **ğŸ¨ Built Visualizations**: You created professional charts and dashboards\n- **ğŸ’¾ Designed Pipeline**: You built a complete ETL (Extract, Transform, Load) workflow\n\n### ğŸš€ Big Data Skills Unlocked\n- **Distributed Computing**: Using Spark for scalable data processing\n- **Data Analysis**: SQL-like operations on large datasets  \n- **Business Intelligence**: Extracting insights from raw data\n- **Data Visualization**: Creating compelling charts and reports\n- **Pipeline Architecture**: Understanding the full data workflow\n\n### ğŸ¯ What's Next - Your Learning Path\n\n#### Beginner (Continue Here)\n1. **ğŸ”„ Run This Again**: Try modifying the analysis - look at different date ranges or products\n2. **ğŸ“ Explore Data**: Check out `/data/user_events.json` and `/data/iot_sensors.csv`\n3. **âš¡ Try Streaming**: Run the event producer to see real-time data\n\n#### Intermediate (Ready for More?)\n1. **ğŸ“š Advanced Tutorial**: Open `01_getting_started.ipynb` for complex analysis\n2. **ğŸ”€ Build Workflows**: Create automated pipelines with Airflow at http://localhost:8080\n3. **ğŸŒŠ Stream Processing**: Set up real-time analytics with Kafka\n\n#### Advanced (Feeling Confident?)\n1. **ğŸ¤– Machine Learning**: Build predictive models with the data\n2. **ğŸ“Š Real-time Dashboards**: Create live monitoring systems\n3. **â˜ï¸ Cloud Deployment**: Scale to production environments\n\n### ğŸ› ï¸ Your Sandbox Tools\n- **Jupyter Lab**: http://localhost:8888 (data science environment)\n- **Airflow**: http://localhost:8080 (workflow orchestration)  \n- **Spark UI**: http://localhost:4040 (job monitoring)\n- **MinIO**: http://localhost:9000 (data storage)\n- **Kafka UI**: http://localhost:9001 (streaming data)\n\n### ğŸ’¡ Pro Tips for Success\n- **Start Small**: Master one tool at a time before combining them\n- **Think in Pipelines**: Always consider the full data flow from source to insight\n- **Monitor Performance**: Use the UIs to understand how your jobs run\n- **Save Your Work**: Document your analyses for future reference\n\n### ğŸ†˜ Need Help?\n- **Examples**: Check `/examples/` for more tutorials\n- **Documentation**: Read the main README.md\n- **Community**: Share your projects and get help\n\n**You're now equipped to handle real-world big data challenges!** ğŸŒŸ\n\nHappy Data Engineering! ğŸš€âœ¨"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}