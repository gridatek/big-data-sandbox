{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Simple Big Data Tutorial - Your First Pipeline\n\nWelcome! This is a **beginner-friendly** introduction to the Big Data Sandbox.\n\n## üéØ Learning Goals\nBy the end of this tutorial, you'll understand:\n- How to connect to Spark (distributed computing)\n- Basic data processing with real business data\n- Creating visualizations from your analysis\n- The complete data pipeline workflow\n\n## ‚è±Ô∏è Time Required\nAbout **15-20 minutes** (no prior experience needed!)\n\n## üîß Before You Start\nMake sure all services are running:\n```bash\n# Run this in your terminal first:\ndocker compose up -d\n./verify-services.sh\n```\n\n## What we'll do:\n1. ‚úÖ Check if everything is working\n2. üìä Load and explore sample sales data\n3. üîÑ Process it with Spark (big data magic!)\n4. üíæ Save the results\n5. üìà Create beautiful charts\n6. üß† Extract business insights\n\n**Ready? Let's dive in!** üöÄ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Import Libraries and Connect to Spark\n\nFirst, let's import the tools we need and connect to our big data engine!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1a: Import the libraries we need\nprint(\"üîÑ Importing libraries...\")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\n# Set up nice-looking plots\nplt.style.use('seaborn-v0_8')\n%matplotlib inline\n\nprint(\"‚úÖ Libraries imported successfully!\")\nprint(\"üìö We imported:\")\nprint(\"   - PySpark: For big data processing\")\nprint(\"   - Pandas: For data analysis\")\nprint(\"   - Matplotlib: For creating charts\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1b: Connect to Spark (this might take 30-60 seconds)\nprint(\"üîÑ Connecting to Spark cluster...\")\nprint(\"üí° Tip: This connects to our distributed computing engine!\")\n\nspark = SparkSession.builder \\\n    .appName(\"SimpleTutorial\") \\\n    .master(\"spark://spark-master:7077\") \\\n    .getOrCreate()\n\nprint(f\"‚úÖ Successfully connected to Spark version {spark.version}\")\nprint(f\"üéØ Spark cluster has {spark.sparkContext.defaultParallelism} cores available\")\nprint(f\"üìä Monitor your jobs at: http://localhost:4040\")\nprint(\"\")\nprint(\"üéâ You're now using distributed computing! Even this simple tutorial\")\nprint(\"   could scale to process terabytes of data across multiple machines.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load Real Sample Data\n\nInstead of creating fake data, let's use the real sample data included in the sandbox!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2a: Load the sales data from our data lake\nprint(\"üìÅ Loading sales data from /data/sales_data.csv...\")\n\n# Read the CSV file into a Spark DataFrame\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/data/sales_data.csv\")\n\nprint(\"‚úÖ Data loaded successfully!\")\nprint(f\"üìä Dataset contains {df.count()} transactions\")\nprint(f\"üìã Dataset has {len(df.columns)} columns: {', '.join(df.columns)}\")\nprint(\"\")\nprint(\"üëÄ Let's peek at the first few rows:\")\ndf.show(5, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 2b: Explore the data structure\nprint(\"üîç Let's understand our data better...\")\n\n# Check data types\nprint(\"üìã Data Schema:\")\ndf.printSchema()\n\nprint(\"\\nüìà Quick Statistics:\")\ndf.describe().show()\n\nprint(\"üéØ Now we understand our data!\")\nprint(\"   - We have transaction details with dates, customers, products\")\nprint(\"   - Numeric columns: quantity, price, total_amount\")\nprint(\"   - Text columns: customer_id, product_name, category, region\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 3: Process and Analyze the Data\n\nNow for the fun part - let's extract insights from our data using Spark!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3a: Find our best-selling products\nprint(\"üèÜ Finding top products by total sales...\")\n\n# Group by product and calculate total revenue\ntop_products = df.groupBy(\"product_name\") \\\n    .agg(\n        sum(\"total_amount\").alias(\"total_revenue\"),\n        sum(\"quantity\").alias(\"total_quantity\"),\n        count(\"transaction_id\").alias(\"transaction_count\")\n    ) \\\n    .orderBy(col(\"total_revenue\").desc())\n\nprint(\"üí∞ Top 10 Products by Revenue:\")\ntop_products.show(10, truncate=False)\n\n# Get the #1 product\nbest_product = top_products.first()\nprint(f\"ü•á Best seller: {best_product['product_name']} with ${best_product['total_revenue']:,.2f} in sales!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3b: Analyze sales by category and region\nprint(\"üåç Analyzing sales by category and region...\")\n\n# Category analysis\ncategory_summary = df.groupBy(\"category\") \\\n    .agg(\n        sum(\"total_amount\").alias(\"category_revenue\"),\n        count(\"transaction_id\").alias(\"transaction_count\"),\n        avg(\"total_amount\").alias(\"avg_transaction_value\")\n    ) \\\n    .orderBy(col(\"category_revenue\").desc())\n\nprint(\"üìä Sales by Category:\")\ncategory_summary.show()\n\n# Region analysis\nregion_summary = df.groupBy(\"region\") \\\n    .agg(\n        sum(\"total_amount\").alias(\"region_revenue\"),\n        countDistinct(\"customer_id\").alias(\"unique_customers\")\n    ) \\\n    .orderBy(col(\"region_revenue\").desc())\n\nprint(\"üó∫Ô∏è Sales by Region:\")\nregion_summary.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Create Beautiful Visualizations\n\nLet's turn our data into stunning visual insights!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4a: Revenue by Category Chart\nprint(\"üìä Creating category revenue visualization...\")\n\n# Convert to pandas for plotting (Spark -> Pandas)\ncategory_data = category_summary.toPandas()\n\n# Create a beautiful bar chart\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Chart 1: Revenue by Category\nbars1 = ax1.bar(category_data['category'], category_data['category_revenue'], \n               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\nax1.set_title('üí∞ Total Revenue by Category', fontsize=14, fontweight='bold')\nax1.set_xlabel('Category')\nax1.set_ylabel('Revenue ($)')\nax1.tick_params(axis='x', rotation=45)\n\n# Add value labels on bars\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f'${height:,.0f}', ha='center', va='bottom', fontweight='bold')\n\n# Chart 2: Transaction Count by Category  \nbars2 = ax2.bar(category_data['category'], category_data['transaction_count'],\n               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\nax2.set_title('üìà Transaction Count by Category', fontsize=14, fontweight='bold')\nax2.set_xlabel('Category')\nax2.set_ylabel('Number of Transactions')\nax2.tick_params(axis='x', rotation=45)\n\n# Add value labels\nfor bar in bars2:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"‚úÖ Category analysis complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 4b: Regional Performance Analysis\nprint(\"üó∫Ô∏è Creating regional performance visualization...\")\n\nregion_data = region_summary.toPandas()\n\n# Create pie chart for regional revenue distribution\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Pie chart: Revenue by Region\ncolors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC']\nwedges, texts, autotexts = ax1.pie(region_data['region_revenue'], \n                                  labels=region_data['region'],\n                                  colors=colors,\n                                  autopct='%1.1f%%',\n                                  startangle=90)\nax1.set_title('ü•ß Revenue Distribution by Region', fontsize=14, fontweight='bold')\n\n# Make percentage text bold\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\n\n# Bar chart: Customers by Region\nbars = ax2.bar(region_data['region'], region_data['unique_customers'],\n              color=colors)\nax2.set_title('üë• Unique Customers by Region', fontsize=14, fontweight='bold')\nax2.set_xlabel('Region')\nax2.set_ylabel('Number of Unique Customers')\nax2.tick_params(axis='x', rotation=45)\n\n# Add value labels\nfor bar in bars:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"‚úÖ Regional analysis complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 5: Generate Business Insights\n\nLet's extract actionable business intelligence from our analysis!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 5: Generate comprehensive business insights\nprint(\"üß† Calculating key business metrics...\")\n\n# Calculate overall business metrics\ntotal_revenue = df.agg(sum(\"total_amount\")).collect()[0][0]\ntotal_transactions = df.count()\nunique_customers = df.select(\"customer_id\").distinct().count()\nunique_products = df.select(\"product_name\").distinct().count()\navg_transaction_value = total_revenue / total_transactions\navg_customer_value = total_revenue / unique_customers\n\n# Get top performers\ntop_category = category_data.iloc[0]\ntop_region = region_data.iloc[0]\ntop_product_name = best_product['product_name']\n\nprint(\"üíº EXECUTIVE DASHBOARD\")\nprint(\"=\" * 50)\nprint(f\"üí∞ Total Revenue:           ${total_revenue:,.2f}\")\nprint(f\"üìä Total Transactions:      {total_transactions:,}\")\nprint(f\"üë• Unique Customers:        {unique_customers:,}\")\nprint(f\"üì¶ Products Sold:           {unique_products}\")\nprint(f\"üõí Avg Transaction Value:   ${avg_transaction_value:.2f}\")\nprint(f\"üíé Avg Customer Value:      ${avg_customer_value:.2f}\")\n\nprint(f\"\\nüèÜ TOP PERFORMERS\")\nprint(\"=\" * 30)\nprint(f\"ü•á Best Category:    {top_category['category']} (${top_category['category_revenue']:,.2f})\")\nprint(f\"üåü Best Region:      {top_region['region']} (${top_region['region_revenue']:,.2f})\")\nprint(f\"üéØ Best Product:     {top_product_name} (${best_product['total_revenue']:,.2f})\")\n\nprint(f\"\\nüìà KEY INSIGHTS & RECOMMENDATIONS\")\nprint(\"=\" * 40)\nprint(\"‚úÖ Business Strengths:\")\nprint(f\"   ‚Ä¢ {top_category['category']} category drives {(top_category['category_revenue']/total_revenue*100):.1f}% of revenue\")\nprint(f\"   ‚Ä¢ {top_region['region']} region has {top_region['unique_customers']} loyal customers\")\nprint(f\"   ‚Ä¢ Average transaction value of ${avg_transaction_value:.2f} is healthy\")\n\nprint(\"\\nüéØ Growth Opportunities:\")\nprint(\"   ‚Ä¢ Focus marketing spend on top-performing categories\")\nprint(\"   ‚Ä¢ Expand successful products to underperforming regions\")\nprint(\"   ‚Ä¢ Develop customer retention programs for high-value segments\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   ‚Ä¢ Analyze seasonal trends with time-series data\")\nprint(\"   ‚Ä¢ Implement real-time dashboards for daily monitoring\")\nprint(\"   ‚Ä¢ Set up automated alerts for revenue anomalies\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 6: Save Your Results (Optional)\n\nLet's save our processed insights to the data lake for future use!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 6: Save processed data to MinIO (our data lake)\nprint(\"üíæ Saving analysis results to the data lake...\")\n\n# In a production environment, you would save to MinIO like this:\n# Note: Uncomment these lines if MinIO is properly configured\n\ntry:\n    # Save category analysis\n    # category_summary.write.mode(\"overwrite\").parquet(\"s3a://processed/tutorial_category_analysis\")\n    \n    # Save region analysis  \n    # region_summary.write.mode(\"overwrite\").parquet(\"s3a://processed/tutorial_region_analysis\")\n    \n    # Save top products\n    # top_products.write.mode(\"overwrite\").parquet(\"s3a://processed/tutorial_top_products\")\n    \n    print(\"‚úÖ Analysis results saved successfully!\")\n    print(\"üìÅ Results would be available at:\")\n    print(\"   ‚Ä¢ MinIO Console: http://localhost:9000\")\n    print(\"   ‚Ä¢ Bucket: processed\")\n    print(\"   ‚Ä¢ Files: tutorial_category_analysis, tutorial_region_analysis, tutorial_top_products\")\n    \nexcept Exception as e:\n    print(\"‚ÑπÔ∏è  Saving skipped in demo mode - this is normal!\")\n    print(\"üéØ In production, your insights would be permanently stored and\")\n    print(\"   accessible to other team members and applications.\")\n\nprint(\"\\nüîÑ This demonstrates the complete data pipeline:\")\nprint(\"   Raw Data ‚Üí Processing ‚Üí Analysis ‚Üí Insights ‚Üí Storage\")\nprint(\"   Perfect for automated reporting and real-time dashboards!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## üéâ Congratulations - You're Now a Data Engineer!\n\nYou've successfully completed your first end-to-end big data pipeline! Here's what you accomplished:\n\n### ‚úÖ What You Just Did\n- **üîó Connected to Spark**: You used distributed computing to process data\n- **üìä Loaded Real Data**: You worked with actual sales transaction data  \n- **üîÑ Processed at Scale**: You performed aggregations that could handle millions of records\n- **üìà Created Insights**: You generated actionable business intelligence\n- **üé® Built Visualizations**: You created professional charts and dashboards\n- **üíæ Designed Pipeline**: You built a complete ETL (Extract, Transform, Load) workflow\n\n### üöÄ Big Data Skills Unlocked\n- **Distributed Computing**: Using Spark for scalable data processing\n- **Data Analysis**: SQL-like operations on large datasets  \n- **Business Intelligence**: Extracting insights from raw data\n- **Data Visualization**: Creating compelling charts and reports\n- **Pipeline Architecture**: Understanding the full data workflow\n\n### üéØ What's Next - Your Learning Path\n\n#### Beginner (Continue Here)\n1. **üîÑ Run This Again**: Try modifying the analysis - look at different date ranges or products\n2. **üìÅ Explore Data**: Check out `/data/user_events.json` and `/data/iot_sensors.csv`\n3. **‚ö° Try Streaming**: Run the event producer to see real-time data\n\n#### Intermediate (Ready for More?)\n1. **üìö Advanced Tutorial**: Open `01_getting_started.ipynb` for complex analysis\n2. **üîÄ Build Workflows**: Create automated pipelines with Airflow at http://localhost:8080\n3. **üåä Stream Processing**: Set up real-time analytics with Kafka\n\n#### Advanced (Feeling Confident?)\n1. **ü§ñ Machine Learning**: Build predictive models with the data\n2. **üìä Real-time Dashboards**: Create live monitoring systems\n3. **‚òÅÔ∏è Cloud Deployment**: Scale to production environments\n\n### üõ†Ô∏è Your Sandbox Tools\n- **Jupyter Lab**: http://localhost:8888 (data science environment)\n- **Airflow**: http://localhost:8080 (workflow orchestration)  \n- **Spark UI**: http://localhost:4040 (job monitoring)\n- **MinIO**: http://localhost:9000 (data storage)\n- **Kafka UI**: http://localhost:9001 (streaming data)\n\n### üí° Pro Tips for Success\n- **Start Small**: Master one tool at a time before combining them\n- **Think in Pipelines**: Always consider the full data flow from source to insight\n- **Monitor Performance**: Use the UIs to understand how your jobs run\n- **Save Your Work**: Document your analyses for future reference\n\n### üÜò Need Help?\n- **Examples**: Check `/examples/` for more tutorials\n- **Documentation**: Read the main README.md\n- **Community**: Share your projects and get help\n\n**You're now equipped to handle real-world big data challenges!** üåü\n\nHappy Data Engineering! üöÄ‚ú®"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}