name: ðŸŒŠ Test Streaming Examples

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'examples/streaming/**'
      - 'kafka/producers/**'
      - 'docker-compose.yml'
      - '.github/workflows/test-streaming-examples.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'examples/streaming/**'
      - 'kafka/producers/**'
  schedule:
    # Run weekly to catch version drift
    - cron: '0 4 * * 1'
  workflow_dispatch:
    inputs:
      streaming_duration:
        description: 'Streaming test duration in seconds'
        required: false
        default: '120'

jobs:
  test-streaming-examples:
    name: ðŸ§ª Streaming Example Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45

    strategy:
      fail-fast: false
      matrix:
        example:
          - name: "kafka_streaming"
            script: "kafka_streaming.py"
            description: "Basic Kafka Consumer with Analytics"
            requires_producer: true
          - name: "spark_streaming"
            script: "spark_streaming.py"
            description: "Spark Structured Streaming"
            requires_producer: true
          - name: "event_pipeline"
            script: "event_pipeline.py"
            description: "Advanced Event Processing Pipeline"
            requires_producer: true
        kafka_version: ["7.5.0", "7.4.0"]
        spark_version: ["3.3.0", "3.4.0"]
        exclude:
          # Reduce matrix size
          - kafka_version: "7.4.0"
            spark_version: "3.4.0"

    env:
      EXAMPLE_NAME: ${{ matrix.example.name }}
      EXAMPLE_SCRIPT: ${{ matrix.example.script }}
      KAFKA_VERSION: ${{ matrix.kafka_version }}
      SPARK_VERSION: ${{ matrix.spark_version }}
      COMPOSE_PROJECT_NAME: bigdata-stream-${{ github.run_id }}-${{ matrix.example.name }}
      STREAMING_DURATION: ${{ github.event.inputs.streaming_duration || '120' }}

    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4

      - name: ðŸ Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: ðŸ“¦ Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyspark==${{ matrix.spark_version }}
          pip install kafka-python pandas numpy

          # Additional dependencies for event pipeline
          if [ "${{ matrix.example.name }}" == "event_pipeline" ]; then
            pip install matplotlib seaborn
          fi

      - name: ðŸ”§ Setup Docker Environment
        run: |
          # Create test environment
          cp .env.example .env
          echo "KAFKA_VERSION=${{ matrix.kafka_version }}" >> .env
          echo "SPARK_VERSION=${{ matrix.spark_version }}" >> .env

          # Update compose file versions
          sed -i "s/confluentinc\/cp-kafka:7.5.0/confluentinc\/cp-kafka:${{ matrix.kafka_version }}/g" compose.yml
          sed -i "s/confluentinc\/cp-zookeeper:7.5.0/confluentinc\/cp-zookeeper:${{ matrix.kafka_version }}/g" compose.yml
          sed -i "s/spark:3.3.0/spark:${{ matrix.spark_version }}/g" compose.yml || true

      - name: ðŸš€ Start Streaming Services
        run: |
          echo "Starting Big Data services for streaming tests..."
          docker compose up -d

          # Extended wait for streaming services
          echo "Waiting for services to initialize..."
          sleep 120

      - name: ðŸ” Verify Streaming Services
        run: |
          echo "Verifying streaming service health..."

          # Check container status
          docker compose ps

          # Critical services for streaming
          services=(
            "http://localhost:9092|Kafka|kafka"
            "http://localhost:8081|Spark Master|spark-master"
            "http://localhost:9001|Kafka UI|kafka-ui"
          )

          for service in "${services[@]}"; do
            url=$(echo $service | cut -d'|' -f1)
            name=$(echo $service | cut -d'|' -f2)
            container=$(echo $service | cut -d'|' -f3)

            echo "Testing $name..."

            # Special handling for Kafka (not HTTP)
            if [ "$name" == "Kafka" ]; then
              timeout 90 bash -c "
                until docker exec \$(docker compose ps -q kafka) kafka-topics --bootstrap-server localhost:9092 --list 2>/dev/null; do
                  echo 'Waiting for Kafka...'
                  sleep 5
                done
              " || {
                echo "âŒ Kafka failed to start"
                docker compose logs kafka
                exit 1
              }
            else
              timeout 90 bash -c "until curl -f $url 2>/dev/null; do sleep 3; done" || {
                echo "âŒ $name failed to start"
                docker compose logs $container
              }
            fi
          done

          # Create test topics
          echo "Creating test topics..."
          docker exec $(docker compose ps -q kafka) kafka-topics \
            --bootstrap-server localhost:9092 \
            --create --topic user-events \
            --partitions 3 --replication-factor 1 || echo "Topic may already exist"

          docker exec $(docker compose ps -q kafka) kafka-topics \
            --bootstrap-server localhost:9092 \
            --create --topic iot-sensors \
            --partitions 2 --replication-factor 1 || echo "Topic may already exist"

      - name: ðŸŽ¯ Start Event Producer
        if: matrix.example.requires_producer
        run: |
          echo "Starting event producer for ${{ matrix.example.description }}..."

          cd kafka/producers

          # Start producer in background with limited events for testing
          python event_producer.py \
            --continuous \
            --topic user-events \
            --rate 5 \
            --brokers localhost:9092 &

          PRODUCER_PID=$!
          echo "PRODUCER_PID=$PRODUCER_PID" >> $GITHUB_ENV

          # Give producer time to start
          sleep 10

          # Verify events are being produced
          echo "Verifying event production..."
          timeout 30 docker exec $(docker compose ps -q kafka) kafka-console-consumer \
            --bootstrap-server localhost:9092 \
            --topic user-events \
            --max-messages 5 \
            --timeout-ms 10000 || echo "Producer verification timeout (may be normal)"

      - name: ðŸŒŠ Execute Streaming Example
        run: |
          echo "ðŸš€ Executing: ${{ matrix.example.description }}"

          cd examples/streaming

          # Create test script wrapper for controlled execution
          cat > test_${{ matrix.example.name }}.py << EOF
          #!/usr/bin/env python3
          import sys
          import signal
          import time
          from threading import Timer

          def timeout_handler(signum, frame):
              print("â° Streaming test timeout reached")
              sys.exit(0)

          def run_streaming_test():
              print("ðŸŒŠ Starting streaming test for $STREAMING_DURATION seconds...")

              # Set up timeout
              signal.signal(signal.SIGALRM, timeout_handler)
              signal.alarm($STREAMING_DURATION)

              try:
                  # Import and run the streaming example
                  if "${{ matrix.example.name }}" == "kafka_streaming":
                      from kafka_streaming import KafkaStreamProcessor
                      processor = KafkaStreamProcessor()
                      processor.run_streaming_analytics(['user-events'], print_interval=15)

                  elif "${{ matrix.example.name }}" == "spark_streaming":
                      from spark_streaming import SparkStreamingProcessor
                      processor = SparkStreamingProcessor()
                      processor.run_streaming_pipeline()

                  elif "${{ matrix.example.name }}" == "event_pipeline":
                      from event_pipeline import EventProcessingPipeline
                      pipeline = EventProcessingPipeline()
                      pipeline.run_event_pipeline()

              except KeyboardInterrupt:
                  print("âœ… Streaming test completed gracefully")
              except Exception as e:
                  print(f"âŒ Streaming test failed: {str(e)}")
                  import traceback
                  traceback.print_exc()
                  sys.exit(1)
              finally:
                  signal.alarm(0)  # Cancel the alarm

          if __name__ == "__main__":
              run_streaming_test()
          EOF

          chmod +x test_${{ matrix.example.name }}.py

          # Run the streaming test
          timeout $((STREAMING_DURATION + 60)) python test_${{ matrix.example.name }}.py || {
            echo "âš ï¸ Streaming example completed (timeout or graceful exit)"
          }

      - name: ðŸ“Š Validate Streaming Results
        run: |
          echo "Validating streaming results for ${{ matrix.example.name }}..."

          # Check Kafka topics and messages
          echo "ðŸ“¡ Kafka Topic Status:"
          docker exec $(docker compose ps -q kafka) kafka-topics \
            --bootstrap-server localhost:9092 \
            --list || echo "Could not list topics"

          # Check consumer group status
          echo "ðŸ‘¥ Consumer Groups:"
          docker exec $(docker compose ps -q kafka) kafka-consumer-groups \
            --bootstrap-server localhost:9092 \
            --list || echo "Could not list consumer groups"

          # Spark streaming validation
          if [[ "${{ matrix.example.name }}" == *"spark"* ]]; then
            echo "âš¡ Spark Applications:"
            curl -s http://localhost:4040/api/v1/applications | head -10 || echo "No Spark applications found"

            echo "ðŸ“Š Spark Streaming Jobs:"
            curl -s http://localhost:4040/api/v1/applications/*/streaming/batches | head -10 || echo "No streaming batches found"
          fi

          echo "âœ… ${{ matrix.example.description }} validation completed!"

      - name: ðŸ›‘ Stop Event Producer
        if: always() && matrix.example.requires_producer
        run: |
          if [ ! -z "$PRODUCER_PID" ]; then
            echo "Stopping event producer (PID: $PRODUCER_PID)..."
            kill $PRODUCER_PID 2>/dev/null || echo "Producer already stopped"
          fi

      - name: ðŸ“ˆ Collect Streaming Metrics
        if: always()
        run: |
          echo "Collecting streaming performance metrics..."

          # Kafka metrics
          echo "ðŸ“¡ Kafka Metrics:"
          docker exec $(docker compose ps -q kafka) kafka-log-dirs \
            --bootstrap-server localhost:9092 \
            --describe --json | head -20 || echo "Could not collect Kafka metrics"

          # Docker resource usage
          echo "ðŸ³ Container Resources:"
          docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

      - name: ðŸ§¹ Cleanup Streaming Environment
        if: always()
        run: |
          echo "Cleaning up streaming test environment..."

          # Save logs for debugging
          mkdir -p /tmp/logs
          docker compose logs > /tmp/logs/streaming-test.log 2>&1 || true

          # Stop streaming processes
          pkill -f "kafka_streaming\|spark_streaming\|event_pipeline" || echo "No streaming processes to kill"

          # Stop and remove containers
          docker compose down -v --remove-orphans

          # Clean up test files
          rm -f examples/streaming/test_*.py

          # Prune Docker resources
          docker system prune -f

      - name: ðŸ“Š Upload Streaming Artifacts
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: streaming-test-logs-${{ matrix.example.name }}-${{ matrix.kafka_version }}
          path: /tmp/logs/
          retention-days: 5

      - name: ðŸ“‹ Generate Streaming Report
        if: always()
        run: |
          echo "## ðŸŒŠ Streaming Example Test Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Example**: ${{ matrix.example.description }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Script**: ${{ matrix.example.script }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Kafka Version**: ${{ matrix.kafka_version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Spark Version**: ${{ matrix.spark_version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration**: ${{ env.STREAMING_DURATION }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp**: $(date -u)" >> $GITHUB_STEP_SUMMARY

  streaming-summary:
    name: ðŸ“‹ Streaming Test Summary
    needs: test-streaming-examples
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: ðŸ“Š Generate Overall Summary
        run: |
          echo "## ðŸŽ¯ Streaming Examples Test Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.test-streaming-examples.result }}" == "success" ]; then
            echo "âœ… All streaming example tests passed!" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### âœ… Verified Streaming Components:" >> $GITHUB_STEP_SUMMARY
            echo "- Kafka consumer with real-time analytics" >> $GITHUB_STEP_SUMMARY
            echo "- Spark Structured Streaming with windowing" >> $GITHUB_STEP_SUMMARY
            echo "- Advanced event processing pipeline" >> $GITHUB_STEP_SUMMARY
            echo "- Cross-version Kafka/Spark compatibility" >> $GITHUB_STEP_SUMMARY
            echo "- Event production and consumption flow" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Some streaming example tests failed." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ”§ Streaming Troubleshooting:" >> $GITHUB_STEP_SUMMARY
            echo "- Check Kafka broker connectivity" >> $GITHUB_STEP_SUMMARY
            echo "- Verify event producer functionality" >> $GITHUB_STEP_SUMMARY
            echo "- Review Spark streaming job logs" >> $GITHUB_STEP_SUMMARY
            echo "- Ensure sufficient test duration" >> $GITHUB_STEP_SUMMARY
            echo "- Check version compatibility matrix" >> $GITHUB_STEP_SUMMARY
          fi