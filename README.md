# ğŸ§° Big Data Sandbox

[![Docker](https://img.shields.io/badge/docker-ready-blue)](https://docker.com)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen)](CONTRIBUTING.md)
[![Discord](https://img.shields.io/discord/123456789)](https://discord.gg/yourlink)

A lightweight **one-command environment** to learn and practice **big data pipelines** with Kafka, Spark, Airflow, and MinIO â€” without the pain of setting everything up manually.

---

## ğŸš€ What is this?

The **Big Data Sandbox** is an open-source project that provides a ready-to-run environment for experimenting with big data tools. It's perfect for:
- Students learning data engineering
- Developers prototyping pipelines
- Educators preparing workshops or demos

**Included tools (MVP):**
- **Kafka** â€“ Streaming events
- **Spark** â€“ Batch & stream processing
- **Airflow** â€“ Workflow orchestration
- **MinIO** â€“ S3-compatible object storage
- **Jupyter** â€“ Interactive notebooks for experiments

---

## ğŸ’¡ Why Big Data Sandbox?

**The Problem:** Setting up a big data environment takes days of configuration, version conflicts, and debugging.

**Our Solution:** Pre-configured, version-tested components that work together out of the box. Focus on learning concepts, not fighting configs.

**What makes this different:**
- âœ… All services pre-integrated (no manual wiring)
- âœ… Realistic sample data included
- âœ… Production-like patterns (not toy examples)
- âœ… Actively maintained with latest stable versions

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow â”‚â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚ Spark  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                â”‚               â”‚
      â–¼                â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Jupyter â”‚     â”‚      MinIO (S3)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Prerequisites

- Docker & Docker Compose installed
- 8GB+ RAM recommended
- 10GB free disk space
- Ports 8080, 8888, 9000, 9092, 4040, 8081 available

---

## âš¡ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/yourname/big-data-sandbox.git
cd big-data-sandbox
```

### 2. Launch the sandbox
```bash
docker compose up -d
```

### 3. Verify all services are running
```bash
docker compose ps
# All services should show as "Up"
```

### 4. Explore the services
- **Airflow** â†’ [http://localhost:8080](http://localhost:8080) (admin/admin)
- **Jupyter** â†’ [http://localhost:8888](http://localhost:8888) (token: `bigdata`)
- **Spark UI** â†’ [http://localhost:4040](http://localhost:4040)
- **MinIO** â†’ [http://localhost:9000](http://localhost:9000) (minioadmin/minioadmin)
- **Kafka Manager** â†’ [http://localhost:9001](http://localhost:9001)

---

## ğŸ“š Learning Resources

**New to Big Data?** We have two learning paths for you:

- **ğŸ““ Interactive Tutorials**: Start with `jupyter/notebooks/01_getting_started.ipynb` for hands-on learning
- **ğŸ› ï¸ Production Examples**: Use `examples/` directory for real-world workflows

**ğŸ‘‰ [Read the complete Learning Guide](LEARNING_GUIDE.md)** for detailed explanations of when to use each approach.

---

## ğŸ“– First Pipeline - Real Example

Try this working example in under 5 minutes:

```bash
# 1. Upload sample data to MinIO
docker exec -it sandbox-minio mc mb local/raw-data 2>/dev/null || true
docker exec -it sandbox-minio mc cp /data/sales_data.csv local/raw-data/

# 2. Produce events to Kafka
docker exec -it sandbox-kafka kafka-console-producer \
  --broker-list localhost:9092 \
  --topic events < data/sample_events.json

# 3. Trigger the ETL pipeline in Airflow
curl -X POST http://localhost:8080/api/v1/dags/sample_etl/dagRuns \
  -H "Content-Type: application/json" \
  -H "Authorization: Basic YWRtaW46YWRtaW4=" \
  -d '{"conf":{}}'

# 4. Check processed results
docker exec -it sandbox-minio mc ls local/processed/
```

See [`examples/quickstart/`](examples/quickstart/) for the full walkthrough.

---

## ğŸ—‚ Project Structure

```
big-data-sandbox/
â”‚â”€â”€ compose.yml              # One-command environment
â”‚â”€â”€ .env.example            # Environment variables template
â”‚â”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/              # Airflow DAG definitions
â”‚   â”œâ”€â”€ plugins/           # Custom operators
â”‚   â””â”€â”€ config/            # Airflow configuration
â”‚â”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/              # Spark applications
â”‚   â””â”€â”€ config/            # Spark configuration
â”‚â”€â”€ kafka/
â”‚   â”œâ”€â”€ config/            # Kafka broker config
â”‚   â””â”€â”€ producers/         # Sample data producers
â”‚â”€â”€ minio/
â”‚   â””â”€â”€ data/              # Initial buckets & data
â”‚â”€â”€ jupyter/
â”‚   â””â”€â”€ notebooks/         # ğŸ““ Interactive learning tutorials
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ sales_data.csv     # Sample sales dataset
â”‚   â”œâ”€â”€ user_events.json   # Sample event stream
â”‚   â””â”€â”€ iot_sensors.csv    # IoT sensor readings
â”‚â”€â”€ examples/
â”‚   â”œâ”€â”€ quickstart/        # ğŸš€ Complete workflow demos
â”‚   â”œâ”€â”€ streaming/         # ğŸŒŠ Production streaming apps
â”‚   â””â”€â”€ batch/             # ğŸ“Š Enterprise ETL & analytics
â””â”€â”€ README.md              # This file
```

---

## ğŸ”§ Troubleshooting

**Services not starting?**
- Check Docker memory allocation: `docker system info | grep Memory`
- Increase Docker memory to at least 6GB in Docker Desktop settings
- View logs: `docker compose logs [service-name]`

**Port conflicts?**
- Check for running services: `lsof -i :8080` (replace with conflicting port)
- Modify ports in `.env` file (copy from `.env.example`)

**Kafka connection issues?**
- Ensure Kafka is fully started: `docker compose logs kafka | grep "started (kafka.server.KafkaServer)"`
- Wait 30 seconds after startup for all services to initialize

**Need help?** Open an issue with your `docker compose logs` output.

---

## ğŸŒ± Roadmap

### Phase 1 - MVP (Current)
- [x] Core services (Kafka, Spark, Airflow, MinIO)
- [x] Docker Compose setup
- [x] Basic documentation
- [x] Jupyter integration
- [ ] Sample datasets & generators

### Phase 2 - Enhanced Learning (Q1 2025)
- [ ] Interactive tutorials in Jupyter
- [ ] Data generators (IoT sensors, web logs, transactions)
- [ ] Video tutorials series
- [ ] Performance monitoring dashboard
- [ ] Additional connectors (PostgreSQL, MongoDB)

### Phase 3 - Production Ready (Q2 2025)
- [ ] Kubernetes deployment (Helm charts)
- [ ] Delta Lake / Iceberg integration
- [ ] Security configurations (Kerberos, SSL)
- [ ] Multi-node Spark cluster option
- [ ] CI/CD pipeline examples

### Phase 4 - Advanced Features (Q3 2025)
- [ ] Machine Learning pipelines (MLflow)
- [ ] Stream processing with Flink
- [ ] Data quality checks (Great Expectations)
- [ ] Cost optimization guides
- [ ] Cloud deployment scripts (AWS/GCP/Azure)

---

## ğŸ‘¥ Community

- **Discord**: [Join our server](https://discord.gg/yourlink) - Get help and share projects
- **Blog**: Read tutorials at [blog.bigdatasandbox.dev](https://blog.bigdatasandbox.dev)
- **Twitter**: Follow [@bigdatasandbox](https://twitter.com/bigdatasandbox) for updates
- **YouTube**: [Video tutorials](https://youtube.com/@bigdatasandbox)

### Success Stories
> "Cut my workshop prep time from 2 days to 30 minutes!" - *University Professor*

> "Finally, a way to test Spark jobs without AWS bills!" - *Startup Developer*

> "Perfect for our internal data engineering bootcamp" - *Fortune 500 Tech Lead*

---

## ğŸ¤ Contributing

Contributions are welcome! We're looking for:
- ğŸ› Bug reports and fixes
- ğŸ“š Documentation improvements
- ğŸ¯ New example pipelines
- ğŸ”§ Performance optimizations
- ğŸŒ Translations

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Contributors
<!-- ALL-CONTRIBUTORS-LIST:START -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

---

## ğŸ“œ License

MIT License - Free to use, modify, and share. See [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with amazing open-source projects:
- Apache Kafka, Spark, and Airflow
- MinIO Object Storage
- Project Jupyter
- Docker & Docker Compose

Special thanks to the data engineering community for feedback and contributions!

---

**Ready to dive in?** Star â­ this repo and start exploring big data in minutes!