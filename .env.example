# Big Data Sandbox Environment Configuration
# Copy this file to .env and customize as needed

# === Service Ports ===
# Modify these if you have port conflicts

# Airflow
AIRFLOW_WEBSERVER_PORT=8080
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin

# Spark
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8081
SPARK_WORKER_WEBUI_PORT=4040

# Kafka
KAFKA_PORT=9092
KAFKA_UI_PORT=9001

# MinIO
MINIO_PORT=9000
MINIO_CONSOLE_PORT=9090
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# Jupyter
JUPYTER_PORT=8888
JUPYTER_TOKEN=bigdata

# PostgreSQL (for Airflow)
POSTGRES_PORT=5432
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# === Resource Limits ===
# Adjust based on your system capacity

# Spark Worker Resources
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2

# Kafka Memory
KAFKA_HEAP_OPTS=-Xmx512M -Xms512M

# === Data Paths ===
# Local directories for persistent storage

DATA_PATH=./data
LOGS_PATH=./logs
NOTEBOOKS_PATH=./jupyter/notebooks

# === Network ===
NETWORK_NAME=bigdata-network

# === Features ===
# Enable/disable optional features

ENABLE_MONITORING=false
ENABLE_SECURITY=false
ENABLE_SAMPLE_DATA=true

# === Advanced Settings ===

# Airflow Configuration
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true

# Spark Configuration
SPARK_SUBMIT_ARGS=--packages org.apache.hadoop:hadoop-aws:3.3.4

# Kafka Configuration
KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1